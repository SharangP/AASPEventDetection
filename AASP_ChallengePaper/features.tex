In order to classify events, our system extracts a set of features from the training data. This set includes the spectral centroid, spectral flux, spectral sparsity,temporal sparsity, loudness, short time energy and Mel-frequency cepstrum coefficients (MFCC). The MFCCs are based on frequency bands equally spaced on the mel scale which approximates the human auditory system's reposnse.  
Each feature highlights different acoustic properties of the signal. Spectral sparsity is expected to be very large for pure sine tones or bells and smaller for sounds with significant ``noise'' characteristics that imply a wide frequency spectrum. Temporal sparsity is large for sounds such as footsteps in relative silence and is useful for indexing and retrieving these types of sounds. Short time energy  is analogous to the volume of the event and the spectral centroid is essentially the center of mass of the spectrum. They are both expected to be reliable indicators of silence. The spectral flux, also called spectral variation, measures how quickly the power spectrum changes. It can be used to determine the timbre of the audio signal. Different permutations of these features will be tested and the best combination will be used for the system training and classification stages of our system. 

Our feature set is computed within a 40ms window with no overlap, which forces our frames to be 40ms long. This limits our precision but increases our accuracy. In order to increase the performance of the system, we will choose features to be calculated over a long period (1s) . This 1s window will slide every 20ms and overlap will be included with the features obtained within the 40ms window to help obtain a more precise feature set for the events. 
